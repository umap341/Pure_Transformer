{"cells":[{"cell_type":"markdown","source":["# PROVIDE OUTPUT FILENAMES"],"metadata":{"id":"Qo72L6Lwfa_E"}},{"cell_type":"code","source":["RANDOM_MIDI = \"output/pure_transformer_random.mid\"        # output file\n","ONE_NOTE_MIDI = \"output/pure_transformer_5.mid\"    # output file\n","\n","# pretrained models (input)\n","MODEL_FILE = 'model_ep4_samples100_batch16.h5'\n","TOKENIZER_FILE = 'tokenizer_samples100_batch16.p'"],"metadata":{"id":"6hV2NSO8e-G5","executionInfo":{"status":"ok","timestamp":1650434405878,"user_tz":420,"elapsed":163,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Environment setup"],"metadata":{"id":"I51XxLHqkfrz"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"m2_TRvCQHHic","executionInfo":{"status":"ok","timestamp":1650434024749,"user_tz":420,"elapsed":4,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1035,"status":"ok","timestamp":1650434026801,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"},"user_tz":420},"outputId":"a7478820-b2a0-491c-b85b-1d01d200bc6e","id":"5tjfOhssHHid"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5zASHjpWHHid","executionInfo":{"status":"ok","timestamp":1650434028302,"user_tz":420,"elapsed":208,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/Shareddrives/295/Code/Pure_Transformer')"]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"id":"98A2lV2RHHid","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650434035851,"user_tz":420,"elapsed":6767,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}},"outputId":"1cc436b5-dc81-466a-8b9a-c918ec58ac68"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: library in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.0.0)\n","Requirement already satisfied: pretty_midi in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.2.9)\n","Requirement already satisfied: tensorflow==2.2.0rc4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.2.0rc4)\n","Requirement already satisfied: folium==0.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.2.1)\n","Requirement already satisfied: imgaug==0.2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.2.5)\n","Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.19.2)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.0.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2.10.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.3.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.17.3)\n","Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.44.0)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.1.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.3.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.37.1)\n","Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2.2.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.14.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.6.3)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1->-r requirements.txt (line 4)) (2.11.3)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.5->-r requirements.txt (line 5)) (0.18.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (2.6.3)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (2.4.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (3.2.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (1.3.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (7.1.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (3.0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5->-r requirements.txt (line 5)) (4.1.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.8.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.3.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0rc4->-r requirements.txt (line 3)) (3.2.0)\n","Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.7/dist-packages (from pretty_midi->-r requirements.txt (line 2)) (1.2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1->-r requirements.txt (line 4)) (2.0.1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jM49VSaVHHie"},"outputs":[],"source":["# !sudo apt-get install python3.8\n","# !pip install -q library\n","# !pip install pretty_midi\n","# !pip install --upgrade pip\n","# !pip install tensorflow==2.2.0rc4\n","# !pip install folium==0.2.1\n","# !pip install imgaug==0.2.5\n","# !pip install -U numpy==1.19.2"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"rQ0zOdnPHHie","executionInfo":{"status":"ok","timestamp":1650434043668,"user_tz":420,"elapsed":3711,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import backend as K\n","import glob\n","import random\n","import pretty_midi\n","import IPython\n","import numpy as np\n","from tqdm.notebook import tnrange, tqdm_notebook, tqdm\n","from random import shuffle, seed\n","import numpy as np\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import Nadam\n","import numpy as np\n","from numpy.random import choice\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aqumPylvXq0"},"outputs":[],"source":["# !wget https://storage.googleapis.com/magentadata/datasets/maestro/v1.0.0/maestro-v1.0.0-midi.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Maf04wQGPCPy"},"outputs":[],"source":["# !unzip maestro-v1.0.0-midi.zip"]},{"cell_type":"markdown","metadata":{"id":"-BHFcA4awsq9"},"source":["# Prepare some functions\n","See its documentation to see the function"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9XE-Om6DP15C","executionInfo":{"status":"ok","timestamp":1650434044204,"user_tz":420,"elapsed":543,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["\n","class NoteTokenizer:\n","    \n","    def __init__(self):\n","      self.notes_to_index = {}\n","      self.index_to_notes = {}\n","      self.num_of_word = 0\n","      self.unique_word = 0\n","      self.notes_freq = {}\n","        \n","    def transform(self,list_array):\n","      \"\"\" Transform a list of note in string into index.\n","      \n","      Parameters\n","      ==========\n","      list_array : list\n","        list of note in string format\n","      \n","      Returns\n","      =======\n","      The transformed list in numpy array.\n","      \n","      \"\"\"\n","      transformed_list = []\n","      for instance in list_array:\n","          transformed_list.append([self.notes_to_index[note] for note in instance])\n","      return np.array(transformed_list, dtype=np.int32)\n"," \n","    def partial_fit(self, notes):\n","        \"\"\" Partial fit on the dictionary of the tokenizer\n","        \n","        Parameters\n","        ==========\n","        notes : list of notes\n","        \n","        \"\"\"\n","        for note in notes:\n","            note_str = ','.join(str(a) for a in note)\n","            if note_str in self.notes_freq:\n","                self.notes_freq[note_str] += 1\n","                self.num_of_word += 1\n","            else:\n","                self.notes_freq[note_str] = 1\n","                self.unique_word += 1\n","                self.num_of_word += 1\n","                self.notes_to_index[note_str], self.index_to_notes[self.unique_word] = self.unique_word, note_str\n","            \n","    def add_new_note(self, note):\n","        \"\"\" Add a new note into the dictionary\n","\n","        Parameters\n","        ==========\n","        note : str\n","          a new note who is not in dictionary.  \n","\n","        \"\"\"\n","        assert note not in self.notes_to_index\n","        self.unique_word += 1\n","        self.notes_to_index[note], self.index_to_notes[self.unique_word] = self.unique_word, note\n","        \n","def generate_batch_song(list_all_midi, batch_music=16, start_index=0, fs=30, seq_len=50, use_tqdm=False):\n","    \"\"\"\n","    Generate Batch music that will be used to be input and output of the neural network\n","    \n","    Parameters\n","    ==========\n","    list_all_midi : list\n","      List of midi files\n","    batch_music : int\n","      A number of music in one batch\n","    start_index : int\n","      The start index to be batched in list_all_midi\n","    fs : int\n","      Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    seq_len : int\n","      The sequence length of the music to be input of neural network\n","    use_tqdm : bool\n","      Whether to use tqdm or not in the function\n","    \n","    Returns\n","    =======\n","    Tuple of input and target neural network\n","    \n","    \"\"\"\n","    \n","    assert len(list_all_midi) >= batch_music\n","    dict_time_notes = generate_dict_time_notes(list_all_midi, batch_music, start_index, fs, use_tqdm=use_tqdm)\n","    \n","    list_musics = process_notes_in_song(dict_time_notes, seq_len)\n","    collected_list_input, collected_list_target = [], []\n","     \n","    for music in list_musics:\n","        list_training, list_target = generate_input_and_target(music, seq_len)\n","        collected_list_input += list_training\n","        collected_list_target += list_target\n","    return collected_list_input, collected_list_target\n","\n","def generate_dict_time_notes(list_all_midi, batch_song = 16, start_index=0, fs=30, use_tqdm=True):\n","    \"\"\" Generate map (dictionary) of music ( in index ) to piano_roll (in np.array)\n","\n","    Parameters\n","    ==========\n","    list_all_midi : list\n","        List of midi files\n","    batch_music : int\n","      A number of music in one batch\n","    start_index : int\n","      The start index to be batched in list_all_midi\n","    fs : int\n","      Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    use_tqdm : bool\n","      Whether to use tqdm or not in the function\n","\n","    Returns\n","    =======\n","    dictionary of music to piano_roll (in np.array)\n","\n","    \"\"\"\n","    assert len(list_all_midi) >= batch_song\n","    \n","    dict_time_notes = {}\n","    process_tqdm_midi = tqdm_notebook(range(start_index, min(start_index + batch_song, len(list_all_midi)))) if use_tqdm else range(start_index,  min(start_index + batch_song, len(list_all_midi)))\n","    for i in process_tqdm_midi:\n","        midi_file_name = list_all_midi[i]\n","        if use_tqdm:\n","            process_tqdm_midi.set_description(\"Processing {}\".format(midi_file_name))\n","        try: # Handle exception on malformat MIDI files\n","            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)\n","            piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n","            piano_roll = piano_midi.get_piano_roll(fs=fs)\n","            dict_time_notes[i] = piano_roll\n","        except Exception as e:\n","            print(e)\n","            print(\"broken file : {}\".format(midi_file_name))\n","            pass\n","    return dict_time_notes\n","\n","def generate_input_and_target(dict_keys_time, seq_len=50):\n","    \"\"\" Generate input and the target of our deep learning for one music.\n","    \n","    Parameters\n","    ==========\n","    dict_keys_time : dict\n","      Dictionary of timestep and notes\n","    seq_len : int\n","      The length of the sequence\n","      \n","    Returns\n","    =======\n","    Tuple of list of input and list of target of neural network.\n","    \n","       \n","    \"\"\"\n","    # Get the start time and end time\n","    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n","    list_training, list_target = [], []\n","    for index_enum, time in enumerate(range(start_time, end_time)):\n","        list_append_training, list_append_target = [], []\n","        start_iterate = 0\n","        flag_target_append = False # flag to append the test list\n","        if index_enum < seq_len:\n","            start_iterate = seq_len - index_enum - 1\n","            for i in range(start_iterate): # add 'e' to the seq list. \n","                list_append_training.append('e')\n","                flag_target_append = True\n","\n","        for i in range(start_iterate,seq_len):\n","            index_enum = time - (seq_len - i - 1)\n","            if index_enum in dict_keys_time:\n","                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n","            else:\n","                list_append_training.append('e')\n","\n","        # add time + 1 to the list_append_target\n","        if time+1 in dict_keys_time:\n","            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n","        else:\n","            list_append_target.append('e')\n","        list_training.append(list_append_training)\n","        list_target.append(list_append_target)\n","    return list_training, list_target\n","\n","def process_notes_in_song(dict_time_notes, seq_len = 50):\n","    \"\"\"\n","    Iterate the dict of piano rolls into dictionary of timesteps and note played\n","    \n","    Parameters\n","    ==========\n","    dict_time_notes : dict\n","      dict contains index of music ( in index ) to piano_roll (in np.array)\n","    seq_len : int\n","      Length of the sequence\n","      \n","    Returns\n","    =======\n","    Dict of timesteps and note played\n","    \"\"\"\n","    list_of_dict_keys_time = []\n","    \n","    for key in dict_time_notes:\n","        sample = dict_time_notes[key]\n","        times = np.unique(np.where(sample > 0)[1])\n","        index = np.where(sample > 0)\n","        dict_keys_time = {}\n","\n","        for time in times:\n","            index_where = np.where(index[1] == time)\n","            notes = index[0][index_where]\n","            dict_keys_time[time] = notes\n","        list_of_dict_keys_time.append(dict_keys_time)\n","    return list_of_dict_keys_time\n","\n"]},{"cell_type":"code","source":["def piano_roll_to_pretty_midi(piano_roll, fs=100, program=0):\n","    '''Convert a Piano Roll array into a PrettyMidi object\n","     with a single instrument.\n","    Parameters\n","    ----------\n","    piano_roll : np.ndarray, shape=(128,frames), dtype=int\n","        Piano roll of one instrument\n","    fs : int\n","        Sampling frequency of the columns, i.e. each column is spaced apart\n","        by ``1./fs`` seconds.\n","    program : int\n","        The program number of the instrument.\n","    Returns\n","    -------\n","    midi_object : pretty_midi.PrettyMIDI\n","        A pretty_midi.PrettyMIDI class instance describing\n","        the piano roll.\n","    '''\n","    notes, frames = piano_roll.shape\n","    pm = pretty_midi.PrettyMIDI()\n","    instrument = pretty_midi.Instrument(program=program)\n","\n","    # pad 1 column of zeros so we can acknowledge inital and ending events\n","    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], 'constant')\n","\n","    # use changes in velocities to find note on / note off events\n","    velocity_changes = np.nonzero(np.diff(piano_roll).T)\n","\n","    # keep track on velocities and note on times\n","    prev_velocities = np.zeros(notes, dtype=int)\n","    note_on_time = np.zeros(notes)\n","\n","    for time, note in zip(*velocity_changes):\n","        # use time + 1 because of padding above\n","        velocity = piano_roll[note, time + 1]\n","        time = time / fs\n","        if velocity > 0:\n","            if prev_velocities[note] == 0:\n","                note_on_time[note] = time\n","                prev_velocities[note] = velocity\n","        else:\n","            pm_note = pretty_midi.Note(\n","                velocity=prev_velocities[note],\n","                pitch=note,\n","                start=note_on_time[note],\n","                end=time)\n","            instrument.notes.append(pm_note)\n","            prev_velocities[note] = 0\n","    pm.instruments.append(instrument)\n","    return pm"],"metadata":{"id":"3GqEI-o0dRvQ","executionInfo":{"status":"ok","timestamp":1650434049460,"user_tz":420,"elapsed":148,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3W60QHdVQF1"},"source":["We will use the code from here for self-attention  \n","https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TQW22YOVUVx"},"outputs":[],"source":["class SeqSelfAttention(tf.keras.layers.Layer):\n","\n","    ATTENTION_TYPE_ADD = 'additive'\n","    ATTENTION_TYPE_MUL = 'multiplicative'\n","\n","    def __init__(self,\n","                 units=32,\n","                 attention_width=None,\n","                 attention_type=ATTENTION_TYPE_ADD,\n","                 return_attention=False,\n","                 history_only=False,\n","                 kernel_initializer='glorot_normal',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 use_additive_bias=True,\n","                 use_attention_bias=True,\n","                 attention_activation=None,\n","                 attention_regularizer_weight=0.0,\n","                 **kwargs):\n","        \"\"\"Layer initialization.\n","        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n","        :param units: The dimension of the vectors that used to calculate the attention weights.\n","        :param attention_width: The width of local attention.\n","        :param attention_type: 'additive' or 'multiplicative'.\n","        :param return_attention: Whether to return the attention weights for visualization.\n","        :param history_only: Only use historical pieces of data.\n","        :param kernel_initializer: The initializer for weight matrices.\n","        :param bias_initializer: The initializer for biases.\n","        :param kernel_regularizer: The regularization for weight matrices.\n","        :param bias_regularizer: The regularization for biases.\n","        :param kernel_constraint: The constraint for weight matrices.\n","        :param bias_constraint: The constraint for biases.\n","        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n","                                  in additive mode.\n","        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n","        :param attention_activation: The activation used for calculating the weights of attention.\n","        :param attention_regularizer_weight: The weights of attention regularizer.\n","        :param kwargs: Parameters for parent class.\n","        \"\"\"\n","        self.supports_masking = True\n","        self.units = units\n","        self.attention_width = attention_width\n","        self.attention_type = attention_type\n","        self.return_attention = return_attention\n","        self.history_only = history_only\n","        if history_only and attention_width is None:\n","            self.attention_width = int(1e9)\n","\n","        self.use_additive_bias = use_additive_bias\n","        self.use_attention_bias = use_attention_bias\n","        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n","        self.bias_initializer = tf.keras.initializers.get(bias_initializer)\n","        self.kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = tf.keras.regularizers.get(bias_regularizer)\n","        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n","        self.bias_constraint = tf.keras.constraints.get(bias_constraint)\n","        self.attention_activation = tf.keras.activations.get(attention_activation)\n","        self.attention_regularizer_weight = attention_regularizer_weight\n","        self._backend = tf.keras.backend.backend()\n","\n","        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            self.Wx, self.Wt, self.bh = None, None, None\n","            self.Wa, self.ba = None, None\n","        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            self.Wa, self.ba = None, None\n","        else:\n","            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n","\n","        super(SeqSelfAttention, self).__init__(**kwargs)\n","\n","    def get_config(self):\n","        config = {\n","            'units': self.units,\n","            'attention_width': self.attention_width,\n","            'attention_type': self.attention_type,\n","            'return_attention': self.return_attention,\n","            'history_only': self.history_only,\n","            'use_additive_bias': self.use_additive_bias,\n","            'use_attention_bias': self.use_attention_bias,\n","            'kernel_initializer': tf.keras.regularizers.serialize(self.kernel_initializer),\n","            'bias_initializer': tf.keras.regularizers.serialize(self.bias_initializer),\n","            'kernel_regularizer': tf.keras.regularizers.serialize(self.kernel_regularizer),\n","            'bias_regularizer': tf.keras.regularizers.serialize(self.bias_regularizer),\n","            'kernel_constraint': tf.keras.constraints.serialize(self.kernel_constraint),\n","            'bias_constraint': tf.keras.constraints.serialize(self.bias_constraint),\n","            'attention_activation': tf.keras.activations.serialize(self.attention_activation),\n","            'attention_regularizer_weight': self.attention_regularizer_weight,\n","        }\n","        base_config = super(SeqSelfAttention, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def build(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape = input_shape[0]\n","        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            self._build_additive_attention(input_shape)\n","        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            self._build_multiplicative_attention(input_shape)\n","        super(SeqSelfAttention, self).build(input_shape)\n","\n","    def _build_additive_attention(self, input_shape):\n","        feature_dim = input_shape[2]\n","\n","        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n","                                  name='{}_Add_Wt'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n","                                  name='{}_Add_Wx'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_additive_bias:\n","            self.bh = self.add_weight(shape=(self.units,),\n","                                      name='{}_Add_bh'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","        self.Wa = self.add_weight(shape=(self.units, 1),\n","                                  name='{}_Add_Wa'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_attention_bias:\n","            self.ba = self.add_weight(shape=(1,),\n","                                      name='{}_Add_ba'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","    def _build_multiplicative_attention(self, input_shape):\n","        feature_dim = input_shape[2]\n","\n","        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n","                                  name='{}_Mul_Wa'.format(self.name),\n","                                  initializer=self.kernel_initializer,\n","                                  regularizer=self.kernel_regularizer,\n","                                  constraint=self.kernel_constraint)\n","        if self.use_attention_bias:\n","            self.ba = self.add_weight(shape=(1,),\n","                                      name='{}_Mul_ba'.format(self.name),\n","                                      initializer=self.bias_initializer,\n","                                      regularizer=self.bias_regularizer,\n","                                      constraint=self.bias_constraint)\n","\n","    def call(self, inputs, mask=None, **kwargs):\n","        if isinstance(inputs, list):\n","            inputs, positions = inputs\n","            positions = K.cast(positions, 'int32')\n","            mask = mask[1]\n","        else:\n","            positions = None\n","\n","        input_len = K.shape(inputs)[1]\n","\n","        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n","            e = self._call_additive_emission(inputs)\n","        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n","            e = self._call_multiplicative_emission(inputs)\n","\n","        if self.attention_activation is not None:\n","            e = self.attention_activation(e)\n","        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n","        if self.attention_width is not None:\n","            ones = tf.ones((input_len, input_len))\n","            if self.history_only:\n","                local = tf.linalg.band_part(\n","                    ones,\n","                    K.minimum(input_len, self.attention_width - 1),\n","                    0,\n","                )\n","            else:\n","                local = tf.linalg.band_part(\n","                    ones,\n","                    K.minimum(input_len, self.attention_width // 2),\n","                    K.minimum(input_len, (self.attention_width - 1) // 2),\n","                )\n","            e = e * K.expand_dims(local, 0)\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            mask = K.expand_dims(mask)\n","            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n","\n","        # a_{t} = \\text{softmax}(e_t)\n","        s = K.sum(e, axis=-1)\n","        s = K.tile(K.expand_dims(s, axis=-1), K.stack([1, 1, input_len]))\n","        a = e / (s + K.epsilon())\n","\n","        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n","        v = K.batch_dot(a, inputs)\n","        if self.attention_regularizer_weight > 0.0:\n","            self.add_loss(self._attention_regularizer(a))\n","\n","        if positions is not None:\n","            pos_num = K.shape(positions)[1]\n","            batch_indices = K.tile(K.expand_dims(K.arange(K.shape(inputs)[0]), axis=-1), K.stack([1, pos_num]))\n","            pos_indices = K.stack([batch_indices, positions], axis=-1)\n","            v = tf.gather_nd(v, pos_indices)\n","            a = tf.gather_nd(a, pos_indices)\n","\n","        if self.return_attention:\n","            return [v, a]\n","        return v\n","\n","    def _call_additive_emission(self, inputs):\n","        input_shape = K.shape(inputs)\n","        batch_size, input_len = input_shape[0], input_shape[1]\n","\n","        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n","        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n","        q = K.tile(K.expand_dims(q, 2), K.stack([1, 1, input_len, 1]))\n","        k = K.tile(K.expand_dims(k, 1), K.stack([1, input_len, 1, 1]))\n","        if self.use_additive_bias:\n","            h = K.tanh(q + k + self.bh)\n","        else:\n","            h = K.tanh(q + k)\n","\n","        # e_{t, t'} = W_a h_{t, t'} + b_a\n","        if self.use_attention_bias:\n","            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n","        else:\n","            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n","        return e\n","\n","    def _call_multiplicative_emission(self, inputs):\n","        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n","        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n","        if self.use_attention_bias:\n","            e = e + self.ba\n","        return e\n","\n","    def compute_output_shape(self, input_shape):\n","        if isinstance(input_shape, list):\n","            input_shape, pos_shape = input_shape\n","            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n","        else:\n","            output_shape = input_shape\n","        if self.return_attention:\n","            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n","            return [output_shape, attention_shape]\n","        return output_shape\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if isinstance(inputs, list):\n","            mask = mask[1]\n","        if self.return_attention:\n","            return [mask, None]\n","        return mask\n","\n","    def _attention_regularizer(self, attention):\n","        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n","        input_len = K.shape(attention)[-1]\n","        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n","            attention,\n","            K.permute_dimensions(attention, (0, 2, 1))) - tf.eye(input_len))) / batch_size\n","\n","    @staticmethod\n","    def get_custom_objects():\n","      return {'SeqSelfAttention': SeqSelfAttention}"]},{"cell_type":"markdown","metadata":{"id":"VCARY5DB2F3Y"},"source":["# Try To load the model and the Tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jw1aCPCXxbwG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650434067868,"user_tz":420,"elapsed":5414,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}},"outputId":"3c76abd1-7b49-4be8-eb9c-2061a563d6c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["os.chdir('/content/drive/Shareddrives/295/Code/Pure_Transformer/pickles')\n","model = tf.keras.models.load_model(MODEL_FILE)\n","note_tokenizer  = pickle.load( open( TOKENIZER_FILE, \"rb\" ) )"]},{"cell_type":"markdown","metadata":{"id":"oLtB11sJ2T2g"},"source":["# Generate Midi Files"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"WUubXZFWmz_l","executionInfo":{"status":"ok","timestamp":1650434073239,"user_tz":420,"elapsed":163,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["os.chdir('/content/drive/Shareddrives/295/Code/Pure_Transformer')\n","def generate_from_random(unique_notes, seq_len=50):\n","  generate = np.random.randint(0,unique_notes,seq_len).tolist()\n","  return generate\n","    \n","def generate_from_one_note(note_tokenizer, new_notes='35'):\n","  generate = [note_tokenizer.notes_to_index['e'] for i in range(49)]\n","  generate += [note_tokenizer.notes_to_index[new_notes]]\n","  return generate"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"auOai0dppfC2","executionInfo":{"status":"ok","timestamp":1650434078948,"user_tz":420,"elapsed":149,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["def generate_notes(generate, model, unique_notes, max_generated=1000, seq_len=50):\n","  for i in tqdm_notebook(range(max_generated), desc='genrt'):\n","    test_input = np.array([generate])[:,i:i+seq_len]\n","    predicted_note = model.predict(test_input)\n","    random_note_pred = choice(unique_notes+1, 1, replace=False, p=predicted_note[0])\n","    generate.append(random_note_pred[0])\n","  return generate"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Z13PBRKZqlKH","executionInfo":{"status":"ok","timestamp":1650434080755,"user_tz":420,"elapsed":240,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"}}},"outputs":[],"source":["def write_midi_file_from_generated(generate, midi_file_name = \"result.mid\", start_index=49, fs=8, max_generated=1000):\n","  note_string = [note_tokenizer.index_to_notes[ind_note] for ind_note in generate]\n","  array_piano_roll = np.zeros((128,max_generated+1), dtype=np.int16)\n","  for index, note in enumerate(note_string[start_index:]):\n","    if note == 'e':\n","      pass\n","    else:\n","      splitted_note = note.split(',')\n","      for j in splitted_note:\n","        array_piano_roll[int(j),index] = 1\n","  generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n","  print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n","  for note in generate_to_midi.instruments[0].notes:\n","    note.velocity = 100\n","  generate_to_midi.write(midi_file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["bfb8010d859f4e49bd0d1f57682b59f2","2c6622812fbf45749ad616ebe9d6c259","a028685fc62143d98a8855a2f6ee740c","187c457963004aaaa2c7b2143a3548af","5cb6aa8b644b4b41aa3ace12c38c5811","0373bccdc03c4c9db820a8066513610d","cc0210767c0243f8a6cab94527177fcd","d992cae7a8534c60a8e6a6d0f762ad1b","d112761d203942478565de39251bc95a","8ea38b2b6c704c9b903e45f9e8123c33","6cf5c2997f7e4abaa896445cfff2a434"]},"executionInfo":{"elapsed":8810,"status":"ok","timestamp":1649285709428,"user":{"displayName":"Uma Rameshgouda Patil","userId":"05896133680025950882"},"user_tz":420},"id":"-PJcnpf8mUKG","outputId":"df542815-0d02-4bdb-f064-e9337d803765"},"outputs":[{"output_type":"display_data","data":{"text/plain":["genrt:   0%|          | 0/200 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfb8010d859f4e49bd0d1f57682b59f2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tempo 209.41504178272965\n"]}],"source":["max_generate = 200\n","unique_notes = note_tokenizer.unique_word\n","seq_len=50\n","generate = generate_from_random(unique_notes, seq_len)\n","generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n","write_midi_file_from_generated(generate, RANDOM_MIDI, start_index=seq_len-1, fs=7, max_generated = max_generate)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["84fcd18d13b1400693df270583650811","4f8e66bb27b148c0bcc46bcdeac1b4a0","60622f9df23c4e24a06e7a131906ffe8","7002d7a23fbf4ac080e0a7de0c5ebee4","7373e56207e042b89083356357d10aac","2b5cdf57baed4a5faa839ad79f010011","27f425f68a65425a8c2e9fdbe5e70afd","8f9b8a6990d841e7bfe995c5e7e9918f","6aaf8268b63a4702b3c4d4b4f2af7190","f17d7cbe584d4c0b935a313d8595b030","5f7855af7ab54666b96bfc4f65b2bb84"]},"executionInfo":{"elapsed":8203,"status":"ok","timestamp":1650434419206,"user":{"displayName":"Akash Jagannathan","userId":"05838487323594916783"},"user_tz":420},"id":"WWr_tXdZQuYX","outputId":"3c2eef86-7a8f-4bbb-e456-8a422f4e63cd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["genrt:   0%|          | 0/300 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84fcd18d13b1400693df270583650811"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tempo 236.4179104477613\n"]}],"source":["max_generate = 300\n","unique_notes = note_tokenizer.unique_word\n","seq_len=50\n","generate = generate_from_one_note(note_tokenizer, '72')\n","generate = generate_notes(generate, model, unique_notes, max_generate, seq_len)\n","write_midi_file_from_generated(generate, ONE_NOTE_MIDI, start_index=seq_len-1, fs=8, max_generated = max_generate)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Pure_Transformer_inference.ipynb","provenance":[{"file_id":"https://github.com/haryoa/note_music_generator/blob/master/MusicGeneratorPianoColab.ipynb","timestamp":1647468019828}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bfb8010d859f4e49bd0d1f57682b59f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c6622812fbf45749ad616ebe9d6c259","IPY_MODEL_a028685fc62143d98a8855a2f6ee740c","IPY_MODEL_187c457963004aaaa2c7b2143a3548af"],"layout":"IPY_MODEL_5cb6aa8b644b4b41aa3ace12c38c5811"}},"2c6622812fbf45749ad616ebe9d6c259":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0373bccdc03c4c9db820a8066513610d","placeholder":"​","style":"IPY_MODEL_cc0210767c0243f8a6cab94527177fcd","value":"genrt: 100%"}},"a028685fc62143d98a8855a2f6ee740c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d992cae7a8534c60a8e6a6d0f762ad1b","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d112761d203942478565de39251bc95a","value":200}},"187c457963004aaaa2c7b2143a3548af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea38b2b6c704c9b903e45f9e8123c33","placeholder":"​","style":"IPY_MODEL_6cf5c2997f7e4abaa896445cfff2a434","value":" 200/200 [00:08&lt;00:00, 36.75it/s]"}},"5cb6aa8b644b4b41aa3ace12c38c5811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0373bccdc03c4c9db820a8066513610d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc0210767c0243f8a6cab94527177fcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d992cae7a8534c60a8e6a6d0f762ad1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d112761d203942478565de39251bc95a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ea38b2b6c704c9b903e45f9e8123c33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cf5c2997f7e4abaa896445cfff2a434":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84fcd18d13b1400693df270583650811":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f8e66bb27b148c0bcc46bcdeac1b4a0","IPY_MODEL_60622f9df23c4e24a06e7a131906ffe8","IPY_MODEL_7002d7a23fbf4ac080e0a7de0c5ebee4"],"layout":"IPY_MODEL_7373e56207e042b89083356357d10aac"}},"4f8e66bb27b148c0bcc46bcdeac1b4a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b5cdf57baed4a5faa839ad79f010011","placeholder":"​","style":"IPY_MODEL_27f425f68a65425a8c2e9fdbe5e70afd","value":"genrt: 100%"}},"60622f9df23c4e24a06e7a131906ffe8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f9b8a6990d841e7bfe995c5e7e9918f","max":300,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6aaf8268b63a4702b3c4d4b4f2af7190","value":300}},"7002d7a23fbf4ac080e0a7de0c5ebee4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f17d7cbe584d4c0b935a313d8595b030","placeholder":"​","style":"IPY_MODEL_5f7855af7ab54666b96bfc4f65b2bb84","value":" 300/300 [00:07&lt;00:00, 38.58it/s]"}},"7373e56207e042b89083356357d10aac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b5cdf57baed4a5faa839ad79f010011":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27f425f68a65425a8c2e9fdbe5e70afd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f9b8a6990d841e7bfe995c5e7e9918f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aaf8268b63a4702b3c4d4b4f2af7190":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f17d7cbe584d4c0b935a313d8595b030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f7855af7ab54666b96bfc4f65b2bb84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}